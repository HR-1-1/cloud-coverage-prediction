{"cells":[{"cell_type":"markdown","id":"e79b4c54","metadata":{"id":"e79b4c54"},"source":["# Solar AI Challenge\n","***\n","### Objective\n","- Given weather data at 1-min frequency for 360 mins, predict the cloud coverage at 30 mins, 60 mins, 90 mins and 120 mins.\n","***\n","### Approach 1 \n","- Feed in the  360-min data into a LSTM as 1-min sequence.\n","- Pass the final hidden state into a decoder LSTM prediciting the weather data as a sequence for next 2 hours at 10-min intervals\n","- Use MSE of the 12 predicted sequences as *Loss criterion*\n","***"]},{"cell_type":"markdown","id":"134ea9c7","metadata":{"id":"134ea9c7"},"source":["## Import Modules"]},{"cell_type":"code","execution_count":null,"id":"fbd2bed8","metadata":{"id":"fbd2bed8"},"outputs":[],"source":["import os\n","import glob\n","from tqdm import trange\n","import pyprind\n","import time"]},{"cell_type":"code","execution_count":null,"id":"abe189ac","metadata":{"id":"abe189ac"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"27db9f6e","metadata":{"id":"27db9f6e"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torchsummary as summary"]},{"cell_type":"code","execution_count":null,"id":"266ef81b","metadata":{"id":"266ef81b"},"outputs":[],"source":["from torch.utils.data.dataloader import DataLoader\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import random_split"]},{"cell_type":"code","execution_count":null,"id":"6fdea934","metadata":{"id":"6fdea934"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"markdown","id":"c2a28652","metadata":{"id":"c2a28652"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"id":"e94889ec","metadata":{"id":"e94889ec"},"outputs":[],"source":["def get_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","    \n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","    \n","def scale_df(df):\n","    x = df.values\n","    scaler = MinMaxScaler()\n","    x_scaled = scaler.fit_transform(x)\n","    df = pd.DataFrame(x_scaled, columns = df.columns)\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"60e7d989","metadata":{"id":"60e7d989","outputId":"c441b4ca-ddf5-4df7-938d-79175c08ed22"},"outputs":[{"data":{"text/plain":["\"\\ndef construct_fnn(input_size, layers):\\n    \\n    layer_list = []\\n    n_in = input_size\\n    p=0.2\\n    \\n    for i in layers:\\n        \\n        if i == 'D' :\\n            layer_list.append(nn.Dropout(p))\\n        elif i == 'R' :\\n            layer_list.append(nn.ReLU())\\n            layer_list.BatchNorm1d(i_last)\\n        elif i == 'T' :\\n            layer_list.append(nn.Tanh())\\n        elif i == 'S' :  \\n            layer_list.append(nn.Sigmoid())\\n        else :    \\n            layer_list.append(nn.Linear(n_in, i))\\n            i_last = i\\n            n_in = i\\n        \\n    return layer_list\\n\""]},"execution_count":725,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","def construct_fnn(input_size, layers):\n","    \n","    layer_list = []\n","    n_in = input_size\n","    p=0.2\n","    \n","    for i in layers:\n","        \n","        if i == 'D' :\n","            layer_list.append(nn.Dropout(p))\n","        elif i == 'R' :\n","            layer_list.append(nn.ReLU())\n","            layer_list.BatchNorm1d(i_last)\n","        elif i == 'T' :\n","            layer_list.append(nn.Tanh())\n","        elif i == 'S' :  \n","            layer_list.append(nn.Sigmoid())\n","        else :    \n","            layer_list.append(nn.Linear(n_in, i))\n","            i_last = i\n","            n_in = i\n","        \n","    return layer_list\n","'''\n","#nn.Sequential(*construct_fnn(10,[100,'T','D',200,'T','D',100,'T',1,'S']))"]},{"cell_type":"code","execution_count":null,"id":"e7728c6a","metadata":{"id":"e7728c6a"},"outputs":[],"source":["def construct_fnn(input_size, output_size, layers):\n","    \n","    layer_list = []\n","    n_in = input_size\n","    n_out = output_size\n","    p=0.2\n","    \n","    for i in layers:\n","        layer_list.append(nn.Linear(n_in, i))\n","        layer_list.append(nn.Tanh())\n","        layer_list.append(nn.BatchNorm1d(i))\n","        layer_list.append(nn.Dropout(p))\n","        n_in = i\n","        \n","    layer_list.append(nn.Linear(layers[-1], n_out)) \n","    layer_list.append(nn.Sigmoid())\n","    return layer_list"]},{"cell_type":"markdown","id":"60a63f46","metadata":{"id":"60a63f46"},"source":["## DataLoader"]},{"cell_type":"code","execution_count":null,"id":"53aff74b","metadata":{"id":"53aff74b"},"outputs":[],"source":["def get_date(date):\n","    M, D = date.split('/')\n","    M = \"0\" + M if len(M)==1 else M\n","    D = \"0\" + D if len(D)==1 else D\n","    return M+D\n","\n","def get_time(time):\n","    M, H = time.split(':')\n","    return M + H + \"00\"\n","\n","def get_scenario(scenario):\n","    scenario = str(scenario)\n","    if len(scenario) == 1:\n","        scenario = \"00\" + scenario\n","    elif len(scenario) == 2:\n","        scenario = \"0\" + scenario\n","    return scenario    "]},{"cell_type":"code","execution_count":null,"id":"ee5405ef","metadata":{"id":"ee5405ef"},"outputs":[],"source":["def process_train(base):\n","    original = pd.read_csv(os.path.join(base, \"train.csv\"))\n","    train = []\n","\n","    for record in trange(len(original)):\n","        data = original.iloc[record]\n","        entry = {}\n","\n","        entry[\"Date\"] = get_date(data[\"DATE (MM/DD)\"])\n","        entry[\"Time\"] = get_time(data[\"MST\"])\n","        entry[\"Global CMP22\"] = data[\"Global CMP22 (vent/cor) [W/m^2]\"]\n","        entry[\"Direct sNIP\"] = data[\"Direct sNIP [W/m^2]\"]\n","        entry[\"Azimuth Angle\"] = data[\"Azimuth Angle [degrees]\"]\n","        entry[\"Tower Dry Bulb Temperature\"] = data[\"Tower Dry Bulb Temp [deg C]\"]\n","        entry[\"Tower Wet Bulb Temperature\"] = data[\"Tower Wet Bulb Temp [deg C]\"]\n","        entry[\"Tower Dew Point Temperature\"] = data[\"Tower Dew Point Temp [deg C]\"]\n","        entry[\"Tower RH\"] = data[\"Tower RH [%]\"]\n","        entry[\"Total Cloud Cover\"] = data[\"Total Cloud Cover [%]\"]\n","        entry[\"Peak Wind Speed\"] = data[\"Peak Wind Speed @ 6ft [m/s]\"]\n","        entry[\"Avgerage Wind Direction\"] = data[\"Avg Wind Direction @ 6ft [deg from N]\"]\n","        entry[\"Station Pressure\"] = data[\"Station Pressure [mBar]\"]\n","        entry[\"Precipitation\"] = data[\"Precipitation (Accumulated) [mm]\"]\n","        entry[\"Snow Depth\"] = data[\"Snow Depth [cm]\"]\n","        entry[\"Moisture\"] = data[\"Moisture\"]\n","        entry[\"Albedo\"] = data[\"Albedo (CMP11)\"]\n","\n","        #filename = os.path.join(base, \"train\", entry[\"Date\"], \"{0}{1}.jpg\".format(entry[\"Date\"], entry[\"Time\"]))\n","        #entry['image'] = filename if os.path.isfile(filename) else None\n","\n","        train.append(entry)\n","\n","    return pd.DataFrame(train)\n","\n","def process_test(base):\n","    \n","    test = pd.read_csv(os.path.join(base, \"test.csv\"))\n","\n","    test = test.iloc[:,1:-1]\n","    test[\"Scenario\"] = test[\"Scenario\"].apply(get_scenario)\n","    test = test.sort_values(by = ['Scenario', 'Time'])\n","    \n","    return test"]},{"cell_type":"code","execution_count":null,"id":"1f6d0fd3","metadata":{"id":"1f6d0fd3"},"outputs":[],"source":["#train = process_train(base = './')\n","#test = process_test(base = './')\n","#test.to_csv('./processed_test.csv')\n","train = pd.read_csv('./processed_train.csv')\n","test = pd.read_csv('./processed_test.csv')\n","tcc_idx = 7"]},{"cell_type":"code","execution_count":null,"id":"da86094e","metadata":{"id":"da86094e","outputId":"54f0cf72-3c8b-479c-9f4f-c425bdf57fc7"},"outputs":[{"data":{"text/plain":["\"\\ntrain = pd.read_csv('./processed_train.csv')\\ntrain = train[train['Total Cloud Cover'] != -1]\\ntrain = train.reset_index(drop=True).iloc[:,1:]\\nidx_f = train[train['Total Cloud Cover'] == -7999].index\\n#print(len(idx_f)/3)\\n\\nfor x in range(int(len(idx_f)/3)):\\n\\n    idx_temp = idx_f[0+x*3:3+x*3]\\n    train.loc[idx_temp, 'Total Cloud Cover'] = (train.loc[idx_temp[0]-5,'Total Cloud Cover'] + train.loc[idx_temp[2]+5, 'Total Cloud Cover'])/2\\n\\ntrain.loc[train[train['Total Cloud Cover'] < 0].index, 'Total Cloud Cover'] = 0\\n\\ntrain_inputs = []\\ntrain_targets = []\\ndates = train.Date.unique()\\n\\nfor m in dates:\\n\\n    scenario = train[train.Date == m].iloc[:,2:]\\n    scenario = scenario.reset_index(drop=True)\\n    #print(scenario)\\n    for i in range(int((len(train[train.Date == m]) -(8*60 + 1))/10)):\\n        train_inputs.append(scenario.iloc[(i*10):(6*60+i*10+1),:].to_numpy(dtype=np.float32))\\n        train_targets.append(scenario.iloc[[i*10+6*60+x*10 for x in range(1,13)],:].to_numpy(dtype=np.float32))\\n    break\\ntrain_inputs = torch.from_numpy(np.asarray(train_inputs)).to(torch.float32)\\ntrain_targets = torch.from_numpy(np.asarray(train_targets)).to(torch.float32)\\ntrain_targets1 = train_targets.reshape((-1,180))\\n\""]},"execution_count":730,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","train = pd.read_csv('./processed_train.csv')\n","train = train[train['Total Cloud Cover'] != -1]\n","train = train.reset_index(drop=True).iloc[:,1:]\n","idx_f = train[train['Total Cloud Cover'] == -7999].index\n","#print(len(idx_f)/3)\n","\n","for x in range(int(len(idx_f)/3)):\n","\n","    idx_temp = idx_f[0+x*3:3+x*3]\n","    train.loc[idx_temp, 'Total Cloud Cover'] = (train.loc[idx_temp[0]-5,'Total Cloud Cover'] + train.loc[idx_temp[2]+5, 'Total Cloud Cover'])/2\n","\n","train.loc[train[train['Total Cloud Cover'] < 0].index, 'Total Cloud Cover'] = 0\n","\n","train_inputs = []\n","train_targets = []\n","dates = train.Date.unique()\n","\n","for m in dates:\n","\n","    scenario = train[train.Date == m].iloc[:,2:]\n","    scenario = scenario.reset_index(drop=True)\n","    #print(scenario)\n","    for i in range(int((len(train[train.Date == m]) -(8*60 + 1))/10)):\n","        train_inputs.append(scenario.iloc[(i*10):(6*60+i*10+1),:].to_numpy(dtype=np.float32))\n","        train_targets.append(scenario.iloc[[i*10+6*60+x*10 for x in range(1,13)],:].to_numpy(dtype=np.float32))\n","    break\n","train_inputs = torch.from_numpy(np.asarray(train_inputs)).to(torch.float32)\n","train_targets = torch.from_numpy(np.asarray(train_targets)).to(torch.float32)\n","train_targets1 = train_targets.reshape((-1,180))\n","'''"]},{"cell_type":"code","execution_count":null,"id":"d933c8ec","metadata":{"id":"d933c8ec"},"outputs":[],"source":["def createDataset_train(path='./processed_train.csv'):\n","\n","    train = pd.read_csv(path)\n","\n","    train = train[train['Total Cloud Cover'] != -1]\n","    train = train.reset_index(drop=True).iloc[:,1:]\n","    idx_f = train[train['Total Cloud Cover'] == -7999].index\n","    #print(len(idx_f)/3)\n","\n","    for x in range(int(len(idx_f)/3)):\n","\n","        idx_temp = idx_f[0+x*3:3+x*3]\n","        train.loc[idx_temp, 'Total Cloud Cover'] = (train.loc[idx_temp[0]-5,'Total Cloud Cover'] + train.loc[idx_temp[2]+5, 'Total Cloud Cover'])/2\n","\n","    train.loc[train[train['Total Cloud Cover'] < 0].index, 'Total Cloud Cover'] = 0\n","    \n","    train_inputs = []\n","    train_targets = []\n","    dates = train.Date.unique()\n","\n","    for m in dates:\n","\n","        scenario = train[train.Date == m].iloc[:,2:]\n","        scenario = scenario.reset_index(drop=True)\n","        scenario = scale_df(scenario)\n","        #print(scenario)\n","        for i in range(int((len(train[train.Date == m]) -(8*60 + 1))/10)):\n","            train_inputs.append(scenario.iloc[(i*10):(6*60+i*10+1),:].to_numpy(dtype=np.float32))\n","            train_targets.append(scenario.iloc[[i*10+6*60+x*10 for x in range(1,13)],:].to_numpy(dtype=np.float32))\n","\n","    train_inputs = torch.from_numpy(np.asarray(train_inputs)).to(torch.float32)\n","    train_targets = torch.from_numpy(np.asarray(train_targets)).to(torch.float32)\n","    train_targets = train_targets.reshape((-1,180))\n","\n","    dataset = TensorDataset(train_inputs, train_targets)\n","    \n","    return dataset \n","\n","def createDataset_test(path='./processed_test.csv'):\n","    \n","    test = pd.read_csv(path)\n","    idx_f = test[test['Total Cloud Cover'] == -7999].index\n","    \n","    for x in range(int(len(idx_f)/3)):\n","    \n","        idx_temp = idx_f[0+x*3:3+x*3]\n","        test.loc[idx_temp, 'Total Cloud Cover'] = (test.loc[idx_temp[0]-5,'Total Cloud Cover'] + test.loc[idx_temp[2]+5, 'Total Cloud Cover'])/2\n","    \n","    test.loc[test[test['Total Cloud Cover'] < 0].index, 'Total Cloud Cover'] = 0\n","    \n","    test_inputs = []\n","    scenarios = test.Scenario.unique()\n","    for scenario in scenarios:\n","\n","        inputs = test.iloc[(scenario-1)*361 : (scenario-1)*361 + 361, 3:]\n","        scenario = scale_df(inputs)\n","        test_inputs.append(inputs.to_numpy(dtype=np.float32))\n","\n","    test_inputs = torch.from_numpy(np.asarray(test_inputs)).to(torch.float32)\n","    #print(test_inputs.size())\n","    test_inputs = TensorDataset(test_inputs)\n","    \n","    return test_inputs\n","    "]},{"cell_type":"code","execution_count":null,"id":"743dd953","metadata":{"id":"743dd953"},"outputs":[],"source":["dataset = createDataset_train()\n","test_ds = createDataset_test()\n","train_ds, val_ds = random_split(dataset, [len(dataset)-500, 500], generator = torch.Generator().manual_seed(42))"]},{"cell_type":"code","execution_count":null,"id":"291ea280","metadata":{"id":"291ea280","outputId":"eb2635d6-bb3d-424a-c76e-99b2aa28694f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([361, 15]) torch.Size([180])\n","torch.Size([361, 15])\n","torch.Size([64, 361, 15])\n"]}],"source":["for xb, yb in train_ds:\n","    print(xb.size(), yb.size())\n","    break\n","for (xb) in test_ds:\n","    print(xb[0].size())\n","    break\n","    \n","test_dl =  DataLoader(test_ds, batch_size = 64, shuffle=True)\n","for xb in (test_dl):\n","    print(xb[0].size())\n","    break"]},{"cell_type":"markdown","id":"78473655","metadata":{"id":"78473655"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"id":"9ec8f68f","metadata":{"id":"9ec8f68f"},"outputs":[],"source":["class EncoderLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers):\n","        super(EncoderLSTM, self).__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        \n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","    \n","    def forward(self, input_tensor):\n","        \n","        out, (hn, cn) = self.lstm(input_tensor)\n","        \n","        return out[:,-1,:]"]},{"cell_type":"code","execution_count":null,"id":"f00fc046","metadata":{"id":"f00fc046"},"outputs":[],"source":["class DecoderLSTM(nn.Module):\n","    def __init__(self, hidden_dim, num_layer):\n","        super(DecoderLSTM, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.device = get_device()\n","        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, bidirectional=False, batch_first=True)\n","        \n","    def forward(self, input_tensor, time):\n","        lstm_outs = {}\n","        hidden = torch.zeros(num_layers, input_tensor.size(0), self.hidden_dim).to(self.device)\n","        cell = torch.zeros(num_layers, input_tensor.size(0), self.hidden_dim).to(self.device)\n","        for t in range(10, time+10, 10):\n","            input_tensor, (hn, cn) = self.lstm(input_tensor, (hidden, cell))\n","            lstm_outs[t] = input_tensor\n","        \n","        return lstm_outs"]},{"cell_type":"code","execution_count":null,"id":"30b2c2cd","metadata":{"id":"30b2c2cd"},"outputs":[],"source":["class Linear(nn.Module):\n","    def __init__(self,input_dim, output_dim, layers):\n","        super(Linear, self).__init__()\n","        layer_list = construct_fnn(input_dim, output_dim, layers)\n","        self.fc = nn.Sequential(*layer_list)\n","        \n","    def forward(self, x):\n","        x = torch.flatten(x, start_dim = 1)\n","        out = self.fc(x)\n","        return out"]},{"cell_type":"code","execution_count":null,"id":"c166ed29","metadata":{"id":"c166ed29"},"outputs":[],"source":["class Pipeline(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, layers):\n","        super(Pipeline, self).__init__()\n","        self.device = get_device()\n","        self.encoder = EncoderLSTM(input_dim, hidden_dim, num_layers)\n","        self.decoder = DecoderLSTM(hidden_dim, num_layers)\n","        self.linear = Linear(hidden_dim, output_dim, layers)\n","        \n","    def forward(self, x, time):\n","        \n","        out = self.encoder(x)\n","        out = out.unsqueeze(1)\n","        outputs = self.decoder(out, time)\n","        final_outputs = torch.empty((x.size(0), 0)).to(self.device)\n","        for i in range(10, time+10, 10):\n","            outputs[i] = outputs[i].squeeze(1)\n","            out = self.linear(outputs[i])\n","            final_outputs = torch.cat((final_outputs, out), dim=1)\n","        \n","        return final_outputs"]},{"cell_type":"code","execution_count":null,"id":"8956dbaa","metadata":{"id":"8956dbaa"},"outputs":[],"source":["input_dim = 15\n","hidden_dim = 32\n","num_layers = 1\n","layers = [64, 128, 64]\n","output_dim = 15\n","time = 120\n","batch_size = 64"]},{"cell_type":"code","execution_count":null,"id":"582ab230","metadata":{"id":"582ab230"},"outputs":[],"source":["encoder = EncoderLSTM(input_dim, hidden_dim, num_layers)\n","decoder = DecoderLSTM(hidden_dim, num_layers)\n","linear = Linear(hidden_dim, output_dim, layers)\n","\n","pipeline = Pipeline(input_dim, hidden_dim, output_dim, num_layers, layers)"]},{"cell_type":"code","execution_count":null,"id":"7cac07a2","metadata":{"id":"7cac07a2","outputId":"2c2f4bd8-0234-4c9d-ad80-f9c1884ae89f"},"outputs":[{"data":{"text/plain":["'\\nx = torch.rand((batch_size, 361, 15))\\nprint(\"Encoder Input Dims : \", x.size())\\nout = encoder(x)\\nprint(\"Encoder Output Dims : \", out.size())\\nout = out.unsqueeze(1)\\nout = decoder(out, time)\\nprint(\"Decoder Output Dims : \", out[10].size())\\nout[10] = out[10].squeeze(1)\\nout[10] = linear(out[10])\\nprint(\"Linear Output Dims : \", out[10].size())\\n\\n\\nprint(\"-----------\")\\nprint(\"PipeLine Input Dims : \", x.size())\\nout = pipeline(x, time)\\nprint(\"PipeLine Output Dims : \", out.size())\\n'"]},"execution_count":740,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","x = torch.rand((batch_size, 361, 15))\n","print(\"Encoder Input Dims : \", x.size())\n","out = encoder(x)\n","print(\"Encoder Output Dims : \", out.size())\n","out = out.unsqueeze(1)\n","out = decoder(out, time)\n","print(\"Decoder Output Dims : \", out[10].size())\n","out[10] = out[10].squeeze(1)\n","out[10] = linear(out[10])\n","print(\"Linear Output Dims : \", out[10].size())\n","\n","\n","print(\"-----------\")\n","print(\"PipeLine Input Dims : \", x.size())\n","out = pipeline(x, time)\n","print(\"PipeLine Output Dims : \", out.size())\n","'''"]},{"cell_type":"code","execution_count":null,"id":"98acd054","metadata":{"id":"98acd054","outputId":"30e0110d-fd11-4a09-cb98-f64aadf92f0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Params : 34.895K\n"]}],"source":["print(f\"Model Params : {sum(p.numel() for p in pipeline.parameters())/1e3}K\")"]},{"cell_type":"code","execution_count":null,"id":"62ad230c","metadata":{"id":"62ad230c"},"outputs":[],"source":["#print(out[0][0:12]) #Corresponds to weather data at 10th Minute"]},{"cell_type":"markdown","id":"0f306a4a","metadata":{"id":"0f306a4a"},"source":["## Trainer"]},{"cell_type":"code","execution_count":null,"id":"0100a528","metadata":{"scrolled":true,"id":"0100a528","outputId":"68d5ef17-3a27-458f-dd36-f8f8fc38da4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([361, 15]) torch.Size([180])\n"]}],"source":["for (x, y) in train_ds:\n","    print(x.size(), y.size())\n","    break"]},{"cell_type":"code","execution_count":null,"id":"67c2ecd0","metadata":{"id":"67c2ecd0"},"outputs":[],"source":["class Trainer():\n","    def __init__(self, train_ds, val_ds, test_ds, batch_size, epoch, lr, checkpoint):\n","        \n","        self.device = get_device()\n","        self.checkpoint = checkpoint\n","         \n","        self.test_ds = test_ds    \n","        self.train_dl, self.val_dl, self.test_dl = self.get_iterator(train_ds, val_ds, test_ds, batch_size)\n","        self.train_size = len(train_ds)\n","        self.val_size = len(val_ds)\n","        \n","        self.model = self.get_model().to(self.device)\n","        self.criterion = self.get_criterion().to(self.device)\n","        self.optimizer = self.get_optimizer(self.model, lr)\n","        \n","        self.train_loss = []\n","        self.val_loss = []\n","        \n","        self.start_epoch = 1\n","        self.end_epoch = epoch\n","        \n","    def get_iterator(self, train_ds, val_ds, test_ds, batch_size=64):\n","        train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True)\n","        val_dl = DataLoader(val_ds, batch_size = batch_size, shuffle=True)\n","        test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle=False)\n","        \n","        return train_dl, val_dl, test_dl\n","    \n","    def get_criterion(self):\n","        return nn.L1Loss()\n","    \n","    def get_optimizer(self, model, lr):\n","        return torch.optim.SGD(model.parameters(), lr)\n","    \n","    def get_model(self):\n","        model = Pipeline(input_dim, hidden_dim, output_dim, num_layers, layers)\n","        return model\n","    \n","    def move_to_device(self):\n","        self.train_dl = DeviceDataLoader(self.train_dl, self.device)\n","        self.val_dl = DeviceDataLoader(self.val_dl, self.device)\n","        to_device(self.model, self.device);\n","    \n","    def save(self, epoch, checkpoint):\n","        torch.save({\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            }, os.path.join(checkpoint, \"model.pth\"))\n","        \n","        torch.save({\n","            'epoch': epoch,\n","            'loss': (self.train_loss, self.val_loss), \n","        }, os.path.join(checkpoint, \"loss.pth\"))\n","        \n","    def load(self, checkpoint):\n","        if os.path.exists(os.path.join(checkpoint, \"model.pth\")):\n","            checkpoints = torch.load(os.path.join(checkpoint, \"model.pth\"), map_location = self.device)\n","            self.model.load_state_dict(checkpoints['model_state_dict'])\n","            self.optimizer.load_state_dict(checkpoints['optimizer_state_dict'])\n","            \n","        if os.path.exists(os.path.join(checkpoint, \"loss.pth\")):\n","            checkpoints = torch.load(os.path.join(checkpoint, \"loss.pth\"), map_location = self.device)\n","            self.train_loss, self.val_loss = checkpoints['loss']\n","            return checkpoints['epoch']\n","        return 0\n","    \n","    def plot_loss(self, epoch, checkpoint):\n","       # assert epoch == len(self.train_loss)\n","        epochs = np.arange(1, len(self.train_loss)+1)\n","        plt.plot(epochs, self.train_loss, label=\"Train Loss\", color=\"blue\")\n","        plt.plot(epochs, self.val_loss, label=\"Val Loss\", color=\"red\")\n","        \n","        plt.title(\"Loss - \" + str(epoch))\n","        plt.xlabel(\"Epochs\")\n","        plt.ylabel(\"Loss\")\n","        plt.legend(loc=\"best\")\n","        \n","        plt.savefig(os.path.join(checkpoint, \"loss.png\"))\n","        plt.close()\n","     \n","    \n","    def train(self):\n","        epoch_loss = 0\n","        \n","        #gc.collect\n","        torch.cuda.empty_cache()\n","        self.model.train()\n","        \n","        with torch.autograd.set_detect_anomaly(True):\n","            bar = pyprind.ProgBar(len(self.train_dl), bar_char = '█')\n","            for index, (inputs, targets) in enumerate(self.train_dl):\n","                inputs = inputs.to(self.device)\n","                targets = targets.to(self.device)\n","                \n","                self.optimizer.zero_grad()\n","                outputs = self.model(inputs, 120)\n","                loss = self.criterion(outputs, targets)\n","                \n","                epoch_loss += loss.item()/len(self.train_dl)\n","                \n","                self.optimizer.step()\n","                \n","                bar.update()\n","                #gc.collect()\n","                torch.cuda.empty_cache()\n","            \n","        return epoch_loss\n","    \n","    @torch.no_grad()\n","    def evaluate(self):\n","        predicted = []\n","        epoch_loss = 0\n","        \n","        #gc.collect\n","        torch.cuda.empty_cache()\n","        \n","        self.model.eval()\n","        bar = pyprind.ProgBar(len(self.val_dl), bar_char='█')\n","        \n","        for idx, (inputs, targets) in enumerate(self.val_dl):\n","            \n","            inputs = inputs.to(self.device)\n","            targets = targets.to(self.device)\n","            \n","            outputs = self.model(inputs, 120)\n","            \n","            loss = self.criterion(outputs, targets)\n","            \n","            epoch_loss += loss.item()/len(self.val_dl)\n","            \n","            bar.update()\n","            #gc.collect()\n","            torch.cuda.empty_cache()\n","            \n","        return epoch_loss\n","    \n","    @torch.no_grad()\n","    def test(self):\n","        #gc.collect\n","        torch.cuda.empty_cache()\n","        self.model.eval()\n","        sample = pd.read_csv('./submission_sample.csv')\n","        df = sample.copy()\n","        for i in range(len(self.test_ds)):\n","            output = trainer.pred_single(self.test_ds[i][0].unsqueeze(0).to(get_device()))\n","            \n","            df.iloc[i,1] = output[0][index[2]].detach().cpu().numpy()*100\n","            df.iloc[i,2] = output[0][index[5]].detach().cpu().numpy()*100\n","            df.iloc[i,3] = output[0][index[8]].detach().cpu().numpy()*100\n","            df.iloc[i,4] = output[0][index[11]].detach().cpu().numpy()*100\n","\n","        df.to_csv(os.path.join(self.checkpoint, 'submission.csv'), index = False)\n","\n","        \n","    \n","    def fit(self,train=True, next=True, test=False):\n","        \n","        if train:\n","            if next:\n","                self.start_epoch = self.load(self.checkpoint)\n","\n","            for epoch in range(self.start_epoch+1, self.end_epoch+1, 1):\n","\n","                print(\"Starting Epoch[{0}/{1}]\".format(epoch, self.end_epoch))\n","\n","                epoch_loss = self.train()\n","\n","                self.train_loss.append(epoch_loss)\n","\n","                #time.sleep(1)\n","                print(\"Training Loss : {}\".format(epoch_loss))\n","\n","                epoch_loss = self.evaluate()\n","\n","                self.val_loss.append(epoch_loss)\n","                print(\"Val Loss : {}\".format(epoch_loss))\n","\n","                self.save(epoch, self.checkpoint)\n","                self.plot_loss(epoch, self.checkpoint)\n","\n","        if test :   \n","            self.test()\n","            \n","        if not(test) and not(train):\n","            print(\"What else do you want me to do? I can sing a song for you :)\")\n","            \n","    def pred_single(self, input):\n","        self.model.eval()\n","        return self.model(input, 120)\n","    \n","        "]},{"cell_type":"markdown","id":"f61fe9f4","metadata":{"id":"f61fe9f4"},"source":["## Training"]},{"cell_type":"markdown","id":"85a48169","metadata":{"id":"85a48169"},"source":["*Model Versions*\n","- v1 : Opt : SGD, Act : Tanh, Epoch : 10, Lr : 1e-3"]},{"cell_type":"code","execution_count":null,"id":"1958e064","metadata":{"id":"1958e064"},"outputs":[],"source":["checkpoint = './checkpoints/v1'"]},{"cell_type":"code","execution_count":null,"id":"bdc78ba3","metadata":{"id":"bdc78ba3"},"outputs":[],"source":["trainer = Trainer(train_ds, val_ds, test_ds, batch_size=batch_size, epoch=10, lr=1e-3, checkpoint=checkpoint)"]},{"cell_type":"code","execution_count":null,"id":"6e33078c","metadata":{"scrolled":true,"id":"6e33078c"},"outputs":[],"source":["trainer.fit(train=False, next=False, test=True)"]},{"cell_type":"markdown","id":"a1997790","metadata":{"id":"a1997790"},"source":["## Predictions"]},{"cell_type":"code","execution_count":null,"id":"44d133eb","metadata":{"id":"44d133eb","outputId":"82984a24-7a0d-4deb-f142-6cbdd65d757a"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 180])\n"]}],"source":["with torch.no_grad():\n","    for xb, yb in val_ds:\n","        \n","        output = trainer.pred_single(xb.unsqueeze(0).to(get_device()))\n","        print((yb.to(get_device()) - output).size())\n","        break\n","    "]},{"cell_type":"code","execution_count":null,"id":"911c2f47","metadata":{"id":"911c2f47","outputId":"349c8a57-c1d2-4f09-f1e1-4e48daaa4892"},"outputs":[{"data":{"text/plain":["[(0.6805456280708313, 0.8865979313850403),\n"," (0.9140076041221619, 0.9278350472450256),\n"," (0.6115224361419678, 0.938144326210022),\n"," (0.7167075276374817, 0.9587628841400146),\n"," (0.7031257748603821, 0.9587628841400146),\n"," (0.7055176496505737, 0.8144329786300659),\n"," (0.7052063941955566, 0.7216494679450989),\n"," (0.7052353024482727, 0.8453608155250549),\n"," (0.7052298188209534, 0.9587628841400146),\n"," (0.7052300572395325, 0.9793814420700073),\n"," (0.7052282094955444, 0.9793814420700073),\n"," (0.7052288055419922, 0.9793814420700073)]"]},"execution_count":749,"metadata":{},"output_type":"execute_result"}],"source":["tcc_idx = 7\n","index = [(tcc_idx + i*15) for i in range(12) ]\n","final_tcc = []\n","for idx in index:\n","    final_tcc.append((output[0][idx].item(),yb[idx].item()))\n","final_tcc"]},{"cell_type":"markdown","id":"9d0fd5f2","metadata":{"id":"9d0fd5f2"},"source":["## Playground"]},{"cell_type":"code","execution_count":null,"id":"d26d0613","metadata":{"id":"d26d0613"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"JustTheWeather.ipynb","provenance":[{"file_id":"1zGZ6EGgKJNuek5ypbu-rsNDHb4J1MqPi","timestamp":1646331756596}]}},"nbformat":4,"nbformat_minor":5}